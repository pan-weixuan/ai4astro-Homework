{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ae9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d4ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(MK=True):\n",
    "    \n",
    "    flux = np.load('/Users/portia/Documents/AAA-College/AI/ai4astro/spec_cnn/flux.npy')\n",
    "    scls = np.load('/Users/portia/Documents/AAA-College/AI/ai4astro/spec_cnn/spectypes.npy')\n",
    "    \n",
    "    unique_classes = np.unique(scls)\n",
    "    print(\"Unique classes:\", unique_classes)\n",
    "    \n",
    "    # If class labels start from 1, convert to start from 0\n",
    "    if unique_classes.min() == 1:\n",
    "        scls = scls - 1\n",
    "        print(\"Classes converted to start from 0:\", np.unique(scls))\n",
    "    \n",
    "    print(f\"Data shape: {flux.shape}, Label shape: {scls.shape}\")\n",
    "    \n",
    "    # Split training and testing sets\n",
    "    fluxTR, fluxTE, clsTR, clsTE = train_test_split(flux, scls, test_size=0.2, random_state=42)\n",
    "    \n",
    "    Xtrain = torch.from_numpy(fluxTR).float()\n",
    "    Xtest = torch.from_numpy(fluxTE).float()\n",
    "    ytrain = torch.from_numpy(clsTR).long()\n",
    "    ytest = torch.from_numpy(clsTE).long()\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    torch_dataset_train = TensorDataset(Xtrain, ytrain)\n",
    "    torch_dataset_test = TensorDataset(Xtest, ytest)\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=torch_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=torch_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return data_loader_train, data_loader_test, clsTR.shape[0], clsTE.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d781fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_class):\n",
    "        super(RNN_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        # Check input dimensions, if 2D, add sequence dimension\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagation\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Take output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dense(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d40c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RNN_module(model, device, num_class, num_epochs, batch_size, learning_rate, train_loader, test_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training mode\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward propagation\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward propagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss and accuracy\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Evaluation mode\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # Accumulate loss and accuracy\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += batch_y.size(0)\n",
    "                test_correct += (predicted == batch_y).sum().item()\n",
    "                \n",
    "                # Collect predictions and targets for confusion matrix\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss / len(train_loader):.4f}, Train Acc: {100 * correct / total:.2f}%, Test Loss: {test_loss / len(test_loader):.4f}, Test Acc: {100 * test_correct / test_total:.2f}%')\n",
    "        \n",
    "        # Save confusion matrix and model every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0 or epoch == num_epochs - 1:\n",
    "            # Calculate confusion matrix and print classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(all_targets, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8978bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Unique classes: [0 1 2]\n",
      "Data shape: (6000, 7781), Label shape: (6000,)\n",
      "Training set size: 4800, Testing set size: 1200\n",
      "Epoch [1/200], Train Loss: 1.0748, Train Acc: 52.25%, Test Loss: 1.0655, Test Acc: 51.17%\n",
      "Epoch [2/200], Train Loss: 1.0537, Train Acc: 51.35%, Test Loss: 1.0460, Test Acc: 50.08%\n",
      "Epoch [3/200], Train Loss: 1.0353, Train Acc: 51.83%, Test Loss: 1.0303, Test Acc: 53.08%\n",
      "Epoch [4/200], Train Loss: 1.0206, Train Acc: 56.08%, Test Loss: 1.0175, Test Acc: 57.67%\n",
      "Epoch [5/200], Train Loss: 1.0073, Train Acc: 60.19%, Test Loss: 1.0050, Test Acc: 60.17%\n",
      "Epoch [6/200], Train Loss: 0.9942, Train Acc: 62.73%, Test Loss: 0.9930, Test Acc: 61.92%\n",
      "Epoch [7/200], Train Loss: 0.9810, Train Acc: 65.08%, Test Loss: 0.9808, Test Acc: 63.00%\n",
      "Epoch [8/200], Train Loss: 0.9685, Train Acc: 66.81%, Test Loss: 0.9698, Test Acc: 63.50%\n",
      "Epoch [9/200], Train Loss: 0.9578, Train Acc: 67.58%, Test Loss: 0.9595, Test Acc: 64.67%\n",
      "Epoch [10/200], Train Loss: 0.9460, Train Acc: 68.67%, Test Loss: 0.9499, Test Acc: 64.83%\n",
      "Epoch [11/200], Train Loss: 0.9362, Train Acc: 69.12%, Test Loss: 0.9411, Test Acc: 65.50%\n",
      "Epoch [12/200], Train Loss: 0.9261, Train Acc: 69.71%, Test Loss: 0.9331, Test Acc: 66.58%\n",
      "Epoch [13/200], Train Loss: 0.9180, Train Acc: 70.42%, Test Loss: 0.9261, Test Acc: 67.25%\n",
      "Epoch [14/200], Train Loss: 0.9095, Train Acc: 71.08%, Test Loss: 0.9190, Test Acc: 67.33%\n",
      "Epoch [15/200], Train Loss: 0.9015, Train Acc: 71.33%, Test Loss: 0.9126, Test Acc: 67.58%\n",
      "Epoch [16/200], Train Loss: 0.8954, Train Acc: 71.98%, Test Loss: 0.9067, Test Acc: 67.92%\n",
      "Epoch [17/200], Train Loss: 0.8876, Train Acc: 72.62%, Test Loss: 0.9017, Test Acc: 68.25%\n",
      "Epoch [18/200], Train Loss: 0.8819, Train Acc: 73.25%, Test Loss: 0.8984, Test Acc: 68.50%\n",
      "Epoch [19/200], Train Loss: 0.8760, Train Acc: 73.98%, Test Loss: 0.8926, Test Acc: 68.92%\n",
      "Epoch [20/200], Train Loss: 0.8691, Train Acc: 74.35%, Test Loss: 0.8881, Test Acc: 69.92%\n",
      "Epoch [21/200], Train Loss: 0.8634, Train Acc: 74.96%, Test Loss: 0.8847, Test Acc: 70.33%\n",
      "Epoch [22/200], Train Loss: 0.8580, Train Acc: 75.42%, Test Loss: 0.8802, Test Acc: 70.50%\n",
      "Epoch [23/200], Train Loss: 0.8535, Train Acc: 76.02%, Test Loss: 0.8767, Test Acc: 70.42%\n",
      "Epoch [24/200], Train Loss: 0.8484, Train Acc: 76.40%, Test Loss: 0.8730, Test Acc: 70.83%\n",
      "Epoch [25/200], Train Loss: 0.8439, Train Acc: 76.79%, Test Loss: 0.8701, Test Acc: 71.25%\n",
      "Epoch [26/200], Train Loss: 0.8393, Train Acc: 77.08%, Test Loss: 0.8665, Test Acc: 70.92%\n",
      "Epoch [27/200], Train Loss: 0.8354, Train Acc: 77.38%, Test Loss: 0.8655, Test Acc: 70.58%\n",
      "Epoch [28/200], Train Loss: 0.8311, Train Acc: 77.60%, Test Loss: 0.8621, Test Acc: 71.33%\n",
      "Epoch [29/200], Train Loss: 0.8273, Train Acc: 77.98%, Test Loss: 0.8592, Test Acc: 71.50%\n",
      "Epoch [30/200], Train Loss: 0.8232, Train Acc: 78.19%, Test Loss: 0.8576, Test Acc: 71.50%\n",
      "Epoch [31/200], Train Loss: 0.8189, Train Acc: 78.35%, Test Loss: 0.8555, Test Acc: 71.50%\n",
      "Epoch [32/200], Train Loss: 0.8164, Train Acc: 78.50%, Test Loss: 0.8530, Test Acc: 71.42%\n",
      "Epoch [33/200], Train Loss: 0.8123, Train Acc: 78.79%, Test Loss: 0.8510, Test Acc: 71.50%\n",
      "Epoch [34/200], Train Loss: 0.8089, Train Acc: 79.02%, Test Loss: 0.8495, Test Acc: 71.83%\n",
      "Epoch [35/200], Train Loss: 0.8053, Train Acc: 79.17%, Test Loss: 0.8472, Test Acc: 71.92%\n",
      "Epoch [36/200], Train Loss: 0.8021, Train Acc: 79.46%, Test Loss: 0.8451, Test Acc: 72.08%\n",
      "Epoch [37/200], Train Loss: 0.7988, Train Acc: 79.60%, Test Loss: 0.8440, Test Acc: 72.25%\n",
      "Epoch [38/200], Train Loss: 0.7969, Train Acc: 79.71%, Test Loss: 0.8427, Test Acc: 72.17%\n",
      "Epoch [39/200], Train Loss: 0.7937, Train Acc: 79.81%, Test Loss: 0.8412, Test Acc: 71.92%\n",
      "Epoch [40/200], Train Loss: 0.7906, Train Acc: 80.21%, Test Loss: 0.8401, Test Acc: 72.25%\n",
      "Epoch [41/200], Train Loss: 0.7883, Train Acc: 80.29%, Test Loss: 0.8391, Test Acc: 72.33%\n",
      "Epoch [42/200], Train Loss: 0.7860, Train Acc: 80.44%, Test Loss: 0.8377, Test Acc: 71.92%\n",
      "Epoch [43/200], Train Loss: 0.7826, Train Acc: 80.60%, Test Loss: 0.8366, Test Acc: 72.25%\n",
      "Epoch [44/200], Train Loss: 0.7802, Train Acc: 80.81%, Test Loss: 0.8354, Test Acc: 71.67%\n",
      "Epoch [45/200], Train Loss: 0.7787, Train Acc: 80.92%, Test Loss: 0.8349, Test Acc: 72.17%\n",
      "Epoch [46/200], Train Loss: 0.7758, Train Acc: 81.04%, Test Loss: 0.8339, Test Acc: 71.92%\n",
      "Epoch [47/200], Train Loss: 0.7741, Train Acc: 81.42%, Test Loss: 0.8325, Test Acc: 72.17%\n",
      "Epoch [48/200], Train Loss: 0.7709, Train Acc: 81.48%, Test Loss: 0.8325, Test Acc: 71.92%\n",
      "Epoch [49/200], Train Loss: 0.7684, Train Acc: 81.54%, Test Loss: 0.8313, Test Acc: 72.17%\n",
      "Epoch [50/200], Train Loss: 0.7667, Train Acc: 81.92%, Test Loss: 0.8302, Test Acc: 72.25%\n",
      "Epoch [51/200], Train Loss: 0.7643, Train Acc: 82.02%, Test Loss: 0.8294, Test Acc: 72.08%\n",
      "Epoch [52/200], Train Loss: 0.7626, Train Acc: 82.23%, Test Loss: 0.8283, Test Acc: 72.33%\n",
      "Epoch [53/200], Train Loss: 0.7603, Train Acc: 82.31%, Test Loss: 0.8278, Test Acc: 72.58%\n",
      "Epoch [54/200], Train Loss: 0.7584, Train Acc: 82.58%, Test Loss: 0.8265, Test Acc: 72.42%\n",
      "Epoch [55/200], Train Loss: 0.7566, Train Acc: 82.75%, Test Loss: 0.8248, Test Acc: 72.83%\n",
      "Epoch [56/200], Train Loss: 0.7542, Train Acc: 82.98%, Test Loss: 0.8250, Test Acc: 72.33%\n",
      "Epoch [57/200], Train Loss: 0.7521, Train Acc: 83.08%, Test Loss: 0.8246, Test Acc: 72.33%\n",
      "Epoch [58/200], Train Loss: 0.7501, Train Acc: 83.17%, Test Loss: 0.8242, Test Acc: 72.08%\n",
      "Epoch [59/200], Train Loss: 0.7479, Train Acc: 83.38%, Test Loss: 0.8235, Test Acc: 72.50%\n",
      "Epoch [60/200], Train Loss: 0.7462, Train Acc: 83.58%, Test Loss: 0.8231, Test Acc: 72.25%\n",
      "Epoch [61/200], Train Loss: 0.7450, Train Acc: 83.79%, Test Loss: 0.8226, Test Acc: 72.25%\n",
      "Epoch [62/200], Train Loss: 0.7430, Train Acc: 84.00%, Test Loss: 0.8220, Test Acc: 72.17%\n",
      "Epoch [63/200], Train Loss: 0.7415, Train Acc: 84.10%, Test Loss: 0.8211, Test Acc: 72.33%\n",
      "Epoch [64/200], Train Loss: 0.7389, Train Acc: 84.12%, Test Loss: 0.8207, Test Acc: 72.42%\n",
      "Epoch [65/200], Train Loss: 0.7373, Train Acc: 84.27%, Test Loss: 0.8202, Test Acc: 72.42%\n",
      "Epoch [66/200], Train Loss: 0.7367, Train Acc: 84.40%, Test Loss: 0.8199, Test Acc: 72.08%\n",
      "Epoch [67/200], Train Loss: 0.7343, Train Acc: 84.54%, Test Loss: 0.8195, Test Acc: 72.25%\n",
      "Epoch [68/200], Train Loss: 0.7313, Train Acc: 84.73%, Test Loss: 0.8188, Test Acc: 72.08%\n",
      "Epoch [69/200], Train Loss: 0.7308, Train Acc: 84.75%, Test Loss: 0.8194, Test Acc: 71.58%\n",
      "Epoch [70/200], Train Loss: 0.7290, Train Acc: 84.88%, Test Loss: 0.8177, Test Acc: 72.00%\n",
      "Epoch [71/200], Train Loss: 0.7277, Train Acc: 84.98%, Test Loss: 0.8169, Test Acc: 72.00%\n",
      "Epoch [72/200], Train Loss: 0.7258, Train Acc: 85.08%, Test Loss: 0.8174, Test Acc: 72.33%\n",
      "Epoch [73/200], Train Loss: 0.7236, Train Acc: 85.21%, Test Loss: 0.8169, Test Acc: 72.25%\n",
      "Epoch [74/200], Train Loss: 0.7227, Train Acc: 85.46%, Test Loss: 0.8161, Test Acc: 72.33%\n",
      "Epoch [75/200], Train Loss: 0.7209, Train Acc: 85.62%, Test Loss: 0.8161, Test Acc: 72.67%\n",
      "Epoch [76/200], Train Loss: 0.7196, Train Acc: 85.71%, Test Loss: 0.8157, Test Acc: 72.50%\n",
      "Epoch [77/200], Train Loss: 0.7182, Train Acc: 85.92%, Test Loss: 0.8143, Test Acc: 72.75%\n",
      "Epoch [78/200], Train Loss: 0.7171, Train Acc: 85.98%, Test Loss: 0.8143, Test Acc: 72.58%\n",
      "Epoch [79/200], Train Loss: 0.7152, Train Acc: 86.15%, Test Loss: 0.8142, Test Acc: 72.50%\n",
      "Epoch [80/200], Train Loss: 0.7139, Train Acc: 86.31%, Test Loss: 0.8141, Test Acc: 72.58%\n",
      "Epoch [81/200], Train Loss: 0.7131, Train Acc: 86.42%, Test Loss: 0.8134, Test Acc: 72.83%\n",
      "Epoch [82/200], Train Loss: 0.7113, Train Acc: 86.46%, Test Loss: 0.8138, Test Acc: 72.42%\n",
      "Epoch [83/200], Train Loss: 0.7098, Train Acc: 86.75%, Test Loss: 0.8133, Test Acc: 72.42%\n",
      "Epoch [84/200], Train Loss: 0.7082, Train Acc: 86.77%, Test Loss: 0.8129, Test Acc: 72.50%\n",
      "Epoch [85/200], Train Loss: 0.7077, Train Acc: 86.88%, Test Loss: 0.8129, Test Acc: 72.42%\n",
      "Epoch [86/200], Train Loss: 0.7063, Train Acc: 87.08%, Test Loss: 0.8126, Test Acc: 72.33%\n",
      "Epoch [87/200], Train Loss: 0.7044, Train Acc: 87.21%, Test Loss: 0.8120, Test Acc: 72.33%\n",
      "Epoch [88/200], Train Loss: 0.7041, Train Acc: 87.33%, Test Loss: 0.8117, Test Acc: 72.67%\n",
      "Epoch [89/200], Train Loss: 0.7027, Train Acc: 87.38%, Test Loss: 0.8116, Test Acc: 72.75%\n",
      "Epoch [90/200], Train Loss: 0.7010, Train Acc: 87.44%, Test Loss: 0.8112, Test Acc: 72.67%\n",
      "Epoch [91/200], Train Loss: 0.7002, Train Acc: 87.58%, Test Loss: 0.8109, Test Acc: 73.17%\n",
      "Epoch [92/200], Train Loss: 0.6981, Train Acc: 87.73%, Test Loss: 0.8112, Test Acc: 72.92%\n",
      "Epoch [93/200], Train Loss: 0.6976, Train Acc: 87.85%, Test Loss: 0.8111, Test Acc: 72.92%\n",
      "Epoch [94/200], Train Loss: 0.6966, Train Acc: 87.98%, Test Loss: 0.8107, Test Acc: 72.75%\n",
      "Epoch [95/200], Train Loss: 0.6949, Train Acc: 88.06%, Test Loss: 0.8113, Test Acc: 72.67%\n",
      "Epoch [96/200], Train Loss: 0.6937, Train Acc: 88.19%, Test Loss: 0.8103, Test Acc: 72.83%\n",
      "Epoch [97/200], Train Loss: 0.6931, Train Acc: 88.23%, Test Loss: 0.8105, Test Acc: 72.83%\n",
      "Epoch [98/200], Train Loss: 0.6919, Train Acc: 88.42%, Test Loss: 0.8110, Test Acc: 72.83%\n",
      "Epoch [99/200], Train Loss: 0.6902, Train Acc: 88.54%, Test Loss: 0.8097, Test Acc: 73.25%\n",
      "Epoch [100/200], Train Loss: 0.6895, Train Acc: 88.52%, Test Loss: 0.8106, Test Acc: 72.75%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68       396\n",
      "           1       0.73      0.68      0.70       418\n",
      "           2       0.89      0.75      0.82       386\n",
      "\n",
      "    accuracy                           0.73      1200\n",
      "   macro avg       0.75      0.73      0.73      1200\n",
      "weighted avg       0.74      0.73      0.73      1200\n",
      "\n",
      "Epoch [101/200], Train Loss: 0.6885, Train Acc: 88.77%, Test Loss: 0.8107, Test Acc: 72.75%\n",
      "Epoch [102/200], Train Loss: 0.6881, Train Acc: 88.83%, Test Loss: 0.8105, Test Acc: 72.92%\n",
      "Epoch [103/200], Train Loss: 0.6864, Train Acc: 88.83%, Test Loss: 0.8102, Test Acc: 72.92%\n",
      "Epoch [104/200], Train Loss: 0.6847, Train Acc: 88.94%, Test Loss: 0.8096, Test Acc: 73.00%\n",
      "Epoch [105/200], Train Loss: 0.6849, Train Acc: 89.04%, Test Loss: 0.8104, Test Acc: 73.00%\n",
      "Epoch [106/200], Train Loss: 0.6832, Train Acc: 89.15%, Test Loss: 0.8102, Test Acc: 72.75%\n",
      "Epoch [107/200], Train Loss: 0.6818, Train Acc: 89.21%, Test Loss: 0.8098, Test Acc: 72.92%\n",
      "Epoch [108/200], Train Loss: 0.6809, Train Acc: 89.25%, Test Loss: 0.8100, Test Acc: 73.08%\n",
      "Epoch [109/200], Train Loss: 0.6799, Train Acc: 89.33%, Test Loss: 0.8097, Test Acc: 72.83%\n",
      "Epoch [110/200], Train Loss: 0.6794, Train Acc: 89.40%, Test Loss: 0.8100, Test Acc: 72.75%\n",
      "Epoch [111/200], Train Loss: 0.6785, Train Acc: 89.42%, Test Loss: 0.8098, Test Acc: 73.00%\n",
      "Epoch [112/200], Train Loss: 0.6766, Train Acc: 89.48%, Test Loss: 0.8095, Test Acc: 73.08%\n",
      "Epoch [113/200], Train Loss: 0.6775, Train Acc: 89.69%, Test Loss: 0.8093, Test Acc: 73.00%\n",
      "Epoch [114/200], Train Loss: 0.6756, Train Acc: 89.71%, Test Loss: 0.8102, Test Acc: 72.50%\n",
      "Epoch [115/200], Train Loss: 0.6747, Train Acc: 89.81%, Test Loss: 0.8105, Test Acc: 72.67%\n",
      "Epoch [116/200], Train Loss: 0.6751, Train Acc: 89.85%, Test Loss: 0.8105, Test Acc: 72.42%\n",
      "Epoch [117/200], Train Loss: 0.6732, Train Acc: 89.96%, Test Loss: 0.8100, Test Acc: 72.42%\n",
      "Epoch [118/200], Train Loss: 0.6726, Train Acc: 90.06%, Test Loss: 0.8099, Test Acc: 72.58%\n",
      "Epoch [119/200], Train Loss: 0.6714, Train Acc: 90.04%, Test Loss: 0.8101, Test Acc: 72.67%\n",
      "Epoch [120/200], Train Loss: 0.6704, Train Acc: 90.21%, Test Loss: 0.8095, Test Acc: 72.67%\n",
      "Epoch [121/200], Train Loss: 0.6698, Train Acc: 90.27%, Test Loss: 0.8094, Test Acc: 72.75%\n",
      "Epoch [122/200], Train Loss: 0.6687, Train Acc: 90.33%, Test Loss: 0.8099, Test Acc: 72.83%\n",
      "Epoch [123/200], Train Loss: 0.6684, Train Acc: 90.46%, Test Loss: 0.8096, Test Acc: 72.92%\n",
      "Epoch [124/200], Train Loss: 0.6675, Train Acc: 90.50%, Test Loss: 0.8092, Test Acc: 73.00%\n",
      "Epoch [125/200], Train Loss: 0.6671, Train Acc: 90.50%, Test Loss: 0.8098, Test Acc: 72.83%\n",
      "Epoch [126/200], Train Loss: 0.6659, Train Acc: 90.56%, Test Loss: 0.8101, Test Acc: 73.00%\n",
      "Epoch [127/200], Train Loss: 0.6651, Train Acc: 90.62%, Test Loss: 0.8099, Test Acc: 72.83%\n",
      "Epoch [128/200], Train Loss: 0.6646, Train Acc: 90.60%, Test Loss: 0.8103, Test Acc: 72.83%\n",
      "Epoch [129/200], Train Loss: 0.6643, Train Acc: 90.67%, Test Loss: 0.8097, Test Acc: 72.92%\n",
      "Epoch [130/200], Train Loss: 0.6626, Train Acc: 90.77%, Test Loss: 0.8103, Test Acc: 72.58%\n",
      "Epoch [131/200], Train Loss: 0.6628, Train Acc: 90.81%, Test Loss: 0.8108, Test Acc: 72.42%\n",
      "Epoch [132/200], Train Loss: 0.6613, Train Acc: 90.88%, Test Loss: 0.8109, Test Acc: 72.58%\n",
      "Epoch [133/200], Train Loss: 0.6609, Train Acc: 91.02%, Test Loss: 0.8100, Test Acc: 72.67%\n",
      "Epoch [134/200], Train Loss: 0.6612, Train Acc: 91.02%, Test Loss: 0.8105, Test Acc: 72.75%\n",
      "Epoch [135/200], Train Loss: 0.6595, Train Acc: 91.10%, Test Loss: 0.8105, Test Acc: 72.33%\n",
      "Epoch [136/200], Train Loss: 0.6592, Train Acc: 91.12%, Test Loss: 0.8105, Test Acc: 72.42%\n",
      "Epoch [137/200], Train Loss: 0.6595, Train Acc: 91.17%, Test Loss: 0.8105, Test Acc: 72.58%\n",
      "Epoch [138/200], Train Loss: 0.6585, Train Acc: 91.25%, Test Loss: 0.8105, Test Acc: 72.50%\n",
      "Epoch [139/200], Train Loss: 0.6573, Train Acc: 91.31%, Test Loss: 0.8101, Test Acc: 72.50%\n",
      "Epoch [140/200], Train Loss: 0.6564, Train Acc: 91.40%, Test Loss: 0.8099, Test Acc: 72.50%\n",
      "Epoch [141/200], Train Loss: 0.6561, Train Acc: 91.46%, Test Loss: 0.8101, Test Acc: 72.50%\n",
      "Epoch [142/200], Train Loss: 0.6552, Train Acc: 91.46%, Test Loss: 0.8100, Test Acc: 72.50%\n",
      "Epoch [143/200], Train Loss: 0.6544, Train Acc: 91.48%, Test Loss: 0.8106, Test Acc: 72.33%\n",
      "Epoch [144/200], Train Loss: 0.6541, Train Acc: 91.50%, Test Loss: 0.8103, Test Acc: 72.25%\n",
      "Epoch [145/200], Train Loss: 0.6535, Train Acc: 91.58%, Test Loss: 0.8104, Test Acc: 72.33%\n",
      "Epoch [146/200], Train Loss: 0.6527, Train Acc: 91.58%, Test Loss: 0.8104, Test Acc: 72.50%\n",
      "Epoch [147/200], Train Loss: 0.6521, Train Acc: 91.67%, Test Loss: 0.8103, Test Acc: 72.33%\n",
      "Epoch [148/200], Train Loss: 0.6512, Train Acc: 91.62%, Test Loss: 0.8104, Test Acc: 72.50%\n",
      "Epoch [149/200], Train Loss: 0.6507, Train Acc: 91.69%, Test Loss: 0.8104, Test Acc: 72.33%\n",
      "Epoch [150/200], Train Loss: 0.6499, Train Acc: 91.77%, Test Loss: 0.8105, Test Acc: 72.42%\n",
      "Epoch [151/200], Train Loss: 0.6511, Train Acc: 91.85%, Test Loss: 0.8107, Test Acc: 72.25%\n",
      "Epoch [152/200], Train Loss: 0.6492, Train Acc: 91.88%, Test Loss: 0.8108, Test Acc: 72.08%\n",
      "Epoch [153/200], Train Loss: 0.6485, Train Acc: 91.90%, Test Loss: 0.8110, Test Acc: 72.33%\n",
      "Epoch [154/200], Train Loss: 0.6478, Train Acc: 91.92%, Test Loss: 0.8107, Test Acc: 72.33%\n",
      "Epoch [155/200], Train Loss: 0.6479, Train Acc: 91.98%, Test Loss: 0.8109, Test Acc: 72.33%\n",
      "Epoch [156/200], Train Loss: 0.6471, Train Acc: 92.00%, Test Loss: 0.8111, Test Acc: 72.58%\n",
      "Epoch [157/200], Train Loss: 0.6466, Train Acc: 92.08%, Test Loss: 0.8110, Test Acc: 72.25%\n",
      "Epoch [158/200], Train Loss: 0.6457, Train Acc: 92.10%, Test Loss: 0.8111, Test Acc: 72.42%\n",
      "Epoch [159/200], Train Loss: 0.6456, Train Acc: 92.12%, Test Loss: 0.8112, Test Acc: 72.25%\n",
      "Epoch [160/200], Train Loss: 0.6450, Train Acc: 92.10%, Test Loss: 0.8118, Test Acc: 72.33%\n",
      "Epoch [161/200], Train Loss: 0.6443, Train Acc: 92.12%, Test Loss: 0.8114, Test Acc: 72.42%\n",
      "Epoch [162/200], Train Loss: 0.6443, Train Acc: 92.19%, Test Loss: 0.8111, Test Acc: 72.33%\n",
      "Epoch [163/200], Train Loss: 0.6434, Train Acc: 92.23%, Test Loss: 0.8114, Test Acc: 72.17%\n",
      "Epoch [164/200], Train Loss: 0.6436, Train Acc: 92.25%, Test Loss: 0.8111, Test Acc: 72.17%\n",
      "Epoch [165/200], Train Loss: 0.6433, Train Acc: 92.25%, Test Loss: 0.8114, Test Acc: 72.17%\n",
      "Epoch [166/200], Train Loss: 0.6423, Train Acc: 92.27%, Test Loss: 0.8114, Test Acc: 72.25%\n",
      "Epoch [167/200], Train Loss: 0.6417, Train Acc: 92.29%, Test Loss: 0.8117, Test Acc: 72.17%\n",
      "Epoch [168/200], Train Loss: 0.6415, Train Acc: 92.35%, Test Loss: 0.8114, Test Acc: 72.33%\n",
      "Epoch [169/200], Train Loss: 0.6401, Train Acc: 92.40%, Test Loss: 0.8115, Test Acc: 72.25%\n",
      "Epoch [170/200], Train Loss: 0.6399, Train Acc: 92.42%, Test Loss: 0.8112, Test Acc: 72.33%\n",
      "Epoch [171/200], Train Loss: 0.6403, Train Acc: 92.48%, Test Loss: 0.8113, Test Acc: 72.17%\n",
      "Epoch [172/200], Train Loss: 0.6403, Train Acc: 92.50%, Test Loss: 0.8117, Test Acc: 72.17%\n",
      "Epoch [173/200], Train Loss: 0.6394, Train Acc: 92.54%, Test Loss: 0.8111, Test Acc: 72.25%\n",
      "Epoch [174/200], Train Loss: 0.6386, Train Acc: 92.54%, Test Loss: 0.8123, Test Acc: 71.92%\n",
      "Epoch [175/200], Train Loss: 0.6395, Train Acc: 92.44%, Test Loss: 0.8109, Test Acc: 72.17%\n",
      "Epoch [176/200], Train Loss: 0.6389, Train Acc: 92.54%, Test Loss: 0.8113, Test Acc: 72.17%\n",
      "Epoch [177/200], Train Loss: 0.6389, Train Acc: 92.56%, Test Loss: 0.8120, Test Acc: 72.08%\n",
      "Epoch [178/200], Train Loss: 0.6378, Train Acc: 92.56%, Test Loss: 0.8111, Test Acc: 72.17%\n",
      "Epoch [179/200], Train Loss: 0.6382, Train Acc: 92.58%, Test Loss: 0.8119, Test Acc: 72.08%\n",
      "Epoch [180/200], Train Loss: 0.6373, Train Acc: 92.62%, Test Loss: 0.8115, Test Acc: 72.00%\n",
      "Epoch [181/200], Train Loss: 0.6369, Train Acc: 92.71%, Test Loss: 0.8120, Test Acc: 72.08%\n",
      "Epoch [182/200], Train Loss: 0.6367, Train Acc: 92.75%, Test Loss: 0.8118, Test Acc: 72.08%\n",
      "Epoch [183/200], Train Loss: 0.6360, Train Acc: 92.77%, Test Loss: 0.8115, Test Acc: 72.25%\n",
      "Epoch [184/200], Train Loss: 0.6359, Train Acc: 92.81%, Test Loss: 0.8119, Test Acc: 72.00%\n",
      "Epoch [185/200], Train Loss: 0.6350, Train Acc: 92.81%, Test Loss: 0.8113, Test Acc: 72.17%\n",
      "Epoch [186/200], Train Loss: 0.6344, Train Acc: 92.94%, Test Loss: 0.8123, Test Acc: 72.00%\n",
      "Epoch [187/200], Train Loss: 0.6341, Train Acc: 92.92%, Test Loss: 0.8118, Test Acc: 72.17%\n",
      "Epoch [188/200], Train Loss: 0.6339, Train Acc: 92.96%, Test Loss: 0.8123, Test Acc: 72.08%\n",
      "Epoch [189/200], Train Loss: 0.6336, Train Acc: 92.96%, Test Loss: 0.8122, Test Acc: 72.00%\n",
      "Epoch [190/200], Train Loss: 0.6332, Train Acc: 93.10%, Test Loss: 0.8124, Test Acc: 71.83%\n",
      "Epoch [191/200], Train Loss: 0.6323, Train Acc: 93.15%, Test Loss: 0.8118, Test Acc: 71.92%\n",
      "Epoch [192/200], Train Loss: 0.6322, Train Acc: 93.21%, Test Loss: 0.8120, Test Acc: 72.08%\n",
      "Epoch [193/200], Train Loss: 0.6317, Train Acc: 93.23%, Test Loss: 0.8122, Test Acc: 72.08%\n",
      "Epoch [194/200], Train Loss: 0.6311, Train Acc: 93.25%, Test Loss: 0.8127, Test Acc: 71.92%\n",
      "Epoch [195/200], Train Loss: 0.6313, Train Acc: 93.19%, Test Loss: 0.8117, Test Acc: 72.17%\n",
      "Epoch [196/200], Train Loss: 0.6309, Train Acc: 93.23%, Test Loss: 0.8122, Test Acc: 72.00%\n",
      "Epoch [197/200], Train Loss: 0.6304, Train Acc: 93.27%, Test Loss: 0.8126, Test Acc: 71.83%\n",
      "Epoch [198/200], Train Loss: 0.6302, Train Acc: 93.23%, Test Loss: 0.8124, Test Acc: 72.00%\n",
      "Epoch [199/200], Train Loss: 0.6306, Train Acc: 93.25%, Test Loss: 0.8127, Test Acc: 72.00%\n",
      "Epoch [200/200], Train Loss: 0.6287, Train Acc: 93.27%, Test Loss: 0.8130, Test Acc: 71.75%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66       396\n",
      "           1       0.70      0.67      0.68       418\n",
      "           2       0.87      0.78      0.82       386\n",
      "\n",
      "    accuracy                           0.72      1200\n",
      "   macro avg       0.73      0.72      0.72      1200\n",
      "weighted avg       0.73      0.72      0.72      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    num_class = 3  \n",
    "    num_epochs = 200  \n",
    "    batch_size = 128  \n",
    "    learning_rate = 0.001  \n",
    "    \n",
    "    # Load data\n",
    "    train_loader, test_loader, train_size, test_size = read_data()\n",
    "    print(f\"Training set size: {train_size}, Testing set size: {test_size}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = RNN_Model(\n",
    "        input_size=7781,  \n",
    "        hidden_size=128,  \n",
    "        num_layers=2,     \n",
    "        num_class=num_class\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    run_RNN_module(\n",
    "        model, device, num_class, num_epochs, batch_size, learning_rate, train_loader, test_loader\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

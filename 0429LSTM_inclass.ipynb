{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1682172-aa25-4952-8d67-b102c1ec8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:7\n",
      "Loading flux and labels from npy files...\n",
      "Flux shape: (6000, 7781)\n",
      "Labels shape: (6000,)\n",
      "Classes: [0 1 2]\n",
      "Epoch [1/100], Train Loss: 0.6896\n",
      "Epoch [2/100], Train Loss: 0.6535\n",
      "Epoch [3/100], Train Loss: 0.5848\n",
      "Epoch [4/100], Train Loss: 0.5253\n",
      "Epoch [5/100], Train Loss: 0.4947\n",
      "Epoch [6/100], Train Loss: 0.4768\n",
      "Epoch [7/100], Train Loss: 0.4630\n",
      "Epoch [8/100], Train Loss: 0.4484\n",
      "Epoch [9/100], Train Loss: 0.4199\n",
      "Epoch [10/100], Train Loss: 0.4055\n",
      "Accuracy: 0.8325\n",
      "Epoch [11/100], Train Loss: 0.3836\n",
      "Epoch [12/100], Train Loss: 0.3751\n",
      "Epoch [13/100], Train Loss: 0.3377\n",
      "Epoch [14/100], Train Loss: 0.3341\n",
      "Epoch [15/100], Train Loss: 0.3220\n",
      "Epoch [16/100], Train Loss: 0.3111\n",
      "Epoch [17/100], Train Loss: 0.2868\n",
      "Epoch [18/100], Train Loss: 0.2611\n",
      "Epoch [19/100], Train Loss: 0.2460\n",
      "Epoch [20/100], Train Loss: 0.2321\n",
      "Accuracy: 0.8500\n",
      "Epoch [21/100], Train Loss: 0.2425\n",
      "Epoch [22/100], Train Loss: 0.2432\n",
      "Epoch [23/100], Train Loss: 0.2172\n",
      "Epoch [24/100], Train Loss: 0.2103\n",
      "Epoch [25/100], Train Loss: 0.2091\n",
      "Epoch [26/100], Train Loss: 0.2068\n",
      "Epoch [27/100], Train Loss: 0.1990\n",
      "Epoch [28/100], Train Loss: 0.2138\n",
      "Epoch [29/100], Train Loss: 0.1790\n",
      "Epoch [30/100], Train Loss: 0.1930\n",
      "Accuracy: 0.7875\n",
      "Epoch [31/100], Train Loss: 0.1727\n",
      "Epoch [32/100], Train Loss: 0.1662\n",
      "Epoch [33/100], Train Loss: 0.1704\n",
      "Epoch [34/100], Train Loss: 0.1543\n",
      "Epoch [35/100], Train Loss: 0.1604\n",
      "Epoch [36/100], Train Loss: 0.1653\n",
      "Epoch [37/100], Train Loss: 0.1471\n",
      "Epoch [38/100], Train Loss: 0.1458\n",
      "Epoch [39/100], Train Loss: 0.1395\n",
      "Epoch [40/100], Train Loss: 0.1280\n",
      "Accuracy: 0.8367\n",
      "Epoch [41/100], Train Loss: 0.1211\n",
      "Epoch [42/100], Train Loss: 0.1147\n",
      "Epoch [43/100], Train Loss: 0.1124\n",
      "Epoch [44/100], Train Loss: 0.1067\n",
      "Epoch [45/100], Train Loss: 0.0965\n",
      "Epoch [46/100], Train Loss: 0.1093\n",
      "Epoch [47/100], Train Loss: 0.1008\n",
      "Epoch [48/100], Train Loss: 0.0904\n",
      "Epoch [49/100], Train Loss: 0.0816\n",
      "Epoch [50/100], Train Loss: 0.0862\n",
      "Accuracy: 0.8183\n",
      "Epoch [51/100], Train Loss: 0.0827\n",
      "Epoch [52/100], Train Loss: 0.0724\n",
      "Epoch [53/100], Train Loss: 0.0708\n",
      "Epoch [54/100], Train Loss: 0.0677\n",
      "Epoch [55/100], Train Loss: 0.0678\n",
      "Epoch [56/100], Train Loss: 0.0521\n",
      "Epoch [57/100], Train Loss: 0.0471\n",
      "Epoch [58/100], Train Loss: 0.0460\n",
      "Epoch [59/100], Train Loss: 0.0539\n",
      "Epoch [60/100], Train Loss: 0.0480\n",
      "Accuracy: 0.8383\n",
      "Epoch [61/100], Train Loss: 0.0525\n",
      "Epoch [62/100], Train Loss: 0.0463\n",
      "Epoch [63/100], Train Loss: 0.0473\n",
      "Epoch [64/100], Train Loss: 0.0431\n",
      "Epoch [65/100], Train Loss: 0.0491\n",
      "Epoch [66/100], Train Loss: 0.0419\n",
      "Epoch [67/100], Train Loss: 0.0358\n",
      "Epoch [68/100], Train Loss: 0.0288\n",
      "Epoch [69/100], Train Loss: 0.0483\n",
      "Epoch [70/100], Train Loss: 0.0309\n",
      "Accuracy: 0.8292\n",
      "Epoch [71/100], Train Loss: 0.0437\n",
      "Epoch [72/100], Train Loss: 0.0463\n",
      "Epoch [73/100], Train Loss: 0.0352\n",
      "Epoch [74/100], Train Loss: 0.0315\n",
      "Epoch [75/100], Train Loss: 0.0281\n",
      "Epoch [76/100], Train Loss: 0.0278\n",
      "Epoch [77/100], Train Loss: 0.0339\n",
      "Epoch [78/100], Train Loss: 0.0401\n",
      "Epoch [79/100], Train Loss: 0.0233\n",
      "Epoch [80/100], Train Loss: 0.0298\n",
      "Accuracy: 0.8417\n",
      "Epoch [81/100], Train Loss: 0.0256\n",
      "Epoch [82/100], Train Loss: 0.0225\n",
      "Epoch [83/100], Train Loss: 0.0231\n",
      "Epoch [84/100], Train Loss: 0.0233\n",
      "Epoch [85/100], Train Loss: 0.0209\n",
      "Epoch [86/100], Train Loss: 0.0184\n",
      "Epoch [87/100], Train Loss: 0.0272\n",
      "Epoch [88/100], Train Loss: 0.0221\n",
      "Epoch [89/100], Train Loss: 0.0185\n",
      "Epoch [90/100], Train Loss: 0.0303\n",
      "Accuracy: 0.8167\n",
      "Epoch [91/100], Train Loss: 0.0258\n",
      "Epoch [92/100], Train Loss: 0.0229\n",
      "Epoch [93/100], Train Loss: 0.0194\n",
      "Epoch [94/100], Train Loss: 0.0203\n",
      "Epoch [95/100], Train Loss: 0.0269\n",
      "Epoch [96/100], Train Loss: 0.0255\n",
      "Epoch [97/100], Train Loss: 0.0327\n",
      "Epoch [98/100], Train Loss: 0.0112\n",
      "Epoch [99/100], Train Loss: 0.0226\n",
      "Epoch [100/100], Train Loss: 0.0147\n",
      "Accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def read_data(flux_path, label_path, batch_size):\n",
    "    print(\"Loading flux and labels from npy files...\")\n",
    "    flux0 = np.load(flux_path).astype(np.float32)  \n",
    "    labels = np.load(label_path, allow_pickle=True)\n",
    "    \n",
    "    print('Flux shape:', flux0.shape)\n",
    "    print('Labels shape:', labels.shape)\n",
    "    \n",
    "    # 处理标签\n",
    "    if len(labels.shape) == 1 or (len(labels.shape) == 2 and labels.shape[1] == 1):\n",
    "        labels = labels.astype(np.int64).flatten()\n",
    "        print(\"Classes:\", np.unique(labels))\n",
    "    \n",
    "    # 填补缺失值\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    flux_imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    flux = flux_imp.fit_transform(flux0)\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    flux = scaler.fit_transform(flux)\n",
    "    \n",
    "    # 数据分割\n",
    "    flux_train, flux_test, label_train, label_test = train_test_split(\n",
    "        flux, labels, test_size=0.2, random_state=42, stratify=labels  # 分层抽样\n",
    "    )\n",
    "    \n",
    "    # 为LSTM重塑数据\n",
    "    seq_len = 25  # 设置序列长度\n",
    "    feature_size = flux.shape[1] // seq_len  # 每个时间步的特征数\n",
    "    \n",
    "    flux_train_reshaped = []\n",
    "    for i in range(flux_train.shape[0]):\n",
    "        seq = []\n",
    "        for j in range(seq_len):\n",
    "            # 处理最后一段可能不足的情况\n",
    "            start_idx = j * feature_size\n",
    "            end_idx = min((j + 1) * feature_size, flux.shape[1])\n",
    "            seq.append(flux_train[i, start_idx:end_idx])\n",
    "        flux_train_reshaped.append(seq)\n",
    "    \n",
    "    flux_test_reshaped = []\n",
    "    for i in range(flux_test.shape[0]):\n",
    "        seq = []\n",
    "        for j in range(seq_len):\n",
    "            start_idx = j * feature_size\n",
    "            end_idx = min((j + 1) * feature_size, flux.shape[1])\n",
    "            seq.append(flux_test[i, start_idx:end_idx])\n",
    "        flux_test_reshaped.append(seq)\n",
    "    \n",
    "    flux_train = np.array(flux_train_reshaped)\n",
    "    flux_test = np.array(flux_test_reshaped)\n",
    "    \n",
    "    # 转换为PyTorch张量\n",
    "    tensor_x_train = torch.from_numpy(flux_train)\n",
    "    tensor_y_train = torch.from_numpy(label_train)\n",
    "    tensor_x_test = torch.from_numpy(flux_test)\n",
    "    tensor_y_test = torch.from_numpy(label_test)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataset = Data.TensorDataset(tensor_x_train, tensor_y_train)\n",
    "    test_dataset = Data.TensorDataset(tensor_x_test, tensor_y_test)\n",
    "    \n",
    "    train_loader = Data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True  # 确保随机洗牌\n",
    "    )\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=3, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        # 添加dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,  # 添加dropout防止过拟合\n",
    "            bidirectional=True  # 使用双向LSTM\n",
    "        )\n",
    "\n",
    "        # 考虑双向LSTM的输出维度\n",
    "        lstm_output_size = hidden_size * 2\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),  # 添加BatchNorm\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM前向传播\n",
    "        output, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 取最后一个时间步\n",
    "        out = output[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_and_evaluate(model, train_loader, test_loader, device, num_epochs=100, learning_rate=0.001):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # 计算平均训练损失\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            accuracy = evaluate(model, test_loader, device)\n",
    "            scheduler.step(train_loss)\n",
    "            \n",
    "            # 保存模型\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    flux_path = './flux.npy'\n",
    "    label_path = './spectypes.npy'\n",
    "    \n",
    "    train_loader, test_loader = read_data(flux_path, label_path, batch_size)\n",
    "    \n",
    "    for x_batch, _ in train_loader:\n",
    "        input_size = x_batch.shape[2]  # [batch_size, seq_len, input_size]\n",
    "        break\n",
    "    \n",
    "\n",
    "    model = LSTM_Model(input_size=input_size).to(device)\n",
    "    train_and_evaluate(model, train_loader, test_loader, device, num_epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
